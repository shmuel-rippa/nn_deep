{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## resourcus\n",
    "* [I am trask blog - simple introduction to NN](http://iamtrask.github.io/)\n",
    "* [Neural bnetwork tutorial](http://www.existor.com/en/news-neural-networks.html) - walk all the way. [A similar tutorial](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)\n",
    "* [Andrew Ng ML course](https://www.coursera.org/learn/machine-learning/)\n",
    "* [Pedro domingos course](https://www.coursera.org/course/machlearning)\n",
    "* [NN papers](https://github.com/robertsdionne/neural-network-papers)\n",
    "* [Brief introduction to deep learning](http://haohanw.blogspot.co.il/2015/01/deep-learning-introduction.html). Based on the [Deep learning lab]()\n",
    "\n",
    "\n",
    "### Courses\n",
    "* [CSC321 Winter 2015: Introduction to NN - Toronto](http://www.cs.toronto.edu/~rgrosse/csc321/calendar.html)\n",
    "\n",
    "### Papers\n",
    "* [Deep learning in Neural Networks: An Overview (2014)](http://arxiv.org/pdf/1404.7828v4.pdf)\n",
    "\n",
    "### Code\n",
    "* http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/ - example of backpropagation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General terminology and notations\n",
    "The notation in here follow the notations in [Chapter 2](http://neuralnetworksanddeeplearning.com/chap2.html) of the deep learning [online book](http://neuralnetworksanddeeplearning.com/index.html). We note that there are different notations in different places, all refer differently to the same mathematical construct.\n",
    "\n",
    "* We consider $L$ layers marked by $l=1,\\ldots,L$ where $l=1$ denotes the input layer\n",
    "* Layer $l$ has $s_l$ units referred to by  $a^{l}_j, j = 1,\\ldots,s_l$.\n",
    "* The matrix $W^l : s_{l} \\times s_{l-1} , l=2,\\ldots, L$ controls the mapping from layer $l-1$ to layer $l$. The vector $\\mathbf{b}^l$ of size $s_{l}$ corresponds to the **bias** term and in layer $l$. The weight $w^l_{ij}$ is the weight associated with the connection of neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$\n",
    "\n",
    "* **Forward propagation:** $\\mathbf{a}^l = \\sigma(\\mathbf{z}^l)$ where $\\mathbf{z}^l=W^l \\mathbf{a}^{(l-1)}+ \\mathbf{b}^{(l)} , l =2,\\ldots,L$ where the activation function $\\sigma \\equiv \\sigma_l$ is applied to each component of its argument vector. For simplicity of notations we often write $\\sigma$ instead of $\\sigma_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonims\n",
    "* **Neuron** - inspired from biology analogy\n",
    "* **Unit** -  It’s one component of a large network\n",
    "* **Feature** - It implements a feature detector that’s looking at the input and will turn on iff the sought feature is present in the input\n",
    "\n",
    "#### A note about dot product\n",
    "The activation function works on the outcome of the done way to think about this is in terms of correlation which are [normalized dot products](https://neonwatty.wordpress.com/2012/07/24/correlation-is-a-normalized-inner-product-illustration-using-the-michael-bay-sian-statistic/). Thus what we really measure is the degree of [correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence), or dependence, between the input vector and the coefficient vector. We can view at the dot product as:\n",
    "* A correlation filter - fires if a correlation between input and weights exceeds a threshold\n",
    "* A feature detector - Detect if a specific pattern occur in the input\n",
    "\n",
    "##  Output unit\n",
    "\n",
    "The output values are computed by similarly multplying the values oh $h$ by another weight matrix,\n",
    "\n",
    "$\\def \\mathbf \\mathbf {}$\n",
    "\n",
    "> $\\mathbf{a}^L = \\sigma_L(\\mathbf{W^L} \\cdot \\mathbf{a}^{(L-1)} + \\mathbf{b}^L) = \\sigma_L(\\mathbf{z^L}) $\n",
    "\n",
    "#### Linear regression network\n",
    "Defined when $\\sigma_L=I$. In that case the output is in a form suitable for linear regression.\n",
    "\n",
    "#### Softmax function\n",
    "\n",
    "For classification problems we want to convert them into probabilities. This is achieved by using the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function.\n",
    "\n",
    "\n",
    "> $\\sigma_L(z) = \\frac{1}{\\alpha}e^z$ where $\\alpha = \\sum_i e^{z^L_i}$ which produces an output vector $\\mathbf{a}^L \\equiv \\mathbf{y} = (y_1,\\ldots,y_{s_L}), y_i = \\frac{e^{z^L_i}}{\\sum_i e^{z^L_i}} , i = 1,\\ldots, s_L$ \n",
    "\n",
    "\n",
    "\n",
    "The element $y_i$ is the probability that the label of the output is $i$. This is indeed the same expression utilized by [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) for classification of many labels. The label $i^*$ that corresponds to a given input vector $\\mathbf{a}^1$ ise selected as the index $i^*$ for which $y_i$ is maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular types of activation functions\n",
    "\n",
    "Example of some popular [activation functions](https://en.wikipedia.org/wiki/Activation_function):\n",
    " * [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function): Transfor inner product into an S shaped curve. There are several popular alternatives for a Sigmoid activation function:\n",
    "   * [The logistic function](https://en.wikipedia.org/wiki/Logistic_function): $\\sigma(z) = \\frac{1}{1+ e^{-z}}$ hase values in [0,1] and thus can be interperable as **probabiliy**.\n",
    "   * [Hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function): $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ with values in $(-1,1)$\n",
    " * [Rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks): $\\sigma(z) = \\max(0,z)$. A unit that user a rectifier function is called a **rectified linear unit (ReLU)**.\n",
    " * [softplus](https://en.wikipedia.org/wiki/Rectifier_(neural_networks): $\\sigma(z) = \\ln (1+e^z)$ is a smooth approximation to the rectifier function. \n",
    " \n",
    "#### Synonyms for the term \"unit activation\"\n",
    "* **Unit's value**: View it as a function of the input\n",
    "* **Activation**: Emphasizes that the unit may be responding or not, or to an extent; it’s most appropriate for logistic units\n",
    "* **Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python example : some activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scalar product between unit and weights  0.2\n",
      " Values of Sigmoid activation function    0.549833997312\n",
      " Values of ta      activation function    0.197375320225\n",
      " Values of sofplus activation function    0.798138869382\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def rectifier(x):\n",
    "    return np.array([max(xv,0.0) for xv in x])\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1.0 + np.exp(x))\n",
    "\n",
    "x     = np.array([1.0,0,0])\n",
    "w     = np.array([0.2,-0.03,0.14])\n",
    "print ' Scalar product between unit and weights ',x.dot(w)\n",
    "print ' Values of Sigmoid activation function   ',sigmoid(x.dot(w))\n",
    "print ' Values of ta      activation function   ',np.tanh(x.dot(w))\n",
    "print ' Values of sofplus activation function   ',softplus(x.dot(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "\n",
    "z  = np.linspace(-2,2,100) # 100 linearly spaced numbers\n",
    "s  = sigmoid(z)    # computing the values of \n",
    "th = np.tanh(z)    # computing the values of \n",
    "re = rectifier(z)  # computing the values of rectifier\n",
    "sp = softplus(z)  # computing the values of rectifier\n",
    "\n",
    "# compose plot\n",
    "pylab.plot(z,s) \n",
    "pylab.plot(z,s,'co',label='Sigmoid')   # Sigmoid \n",
    "pylab.plot(z,th,label='tanh')          # tanh\n",
    "pylab.plot(z,re,label='rectifier')     # rectifier\n",
    "pylab.plot(z,sp,label='softplut')     # rectifier\n",
    "pylab.legend()\n",
    "pylab.show() # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python example : Simple feed forward classification NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  z1  [ 1.2   1.01  1.14]\n",
      " a1  [ 0.83365461  0.76576202  0.81441409]\n",
      " z2  [-0.09339804  1.03365429  0.10293268]\n",
      " y   [ 0.18855564  0.58198547  0.22945889]\n",
      " Input vector [ 1.  0.  0.] is classified to label 1 \n",
      "\n",
      "\n",
      "The probablity for classifying to label  0  is  0.188555644067\n",
      "The probablity for classifying to label  1  is  0.581985469162\n",
      "The probablity for classifying to label  2  is  0.229458886771\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    alpha = np.sum(np.exp(z))\n",
    "    return np.exp(z)/alpha\n",
    "\n",
    "# Input\n",
    "a0  = np.array([1.,0,0])\n",
    "\n",
    "# First layer\n",
    "W1   = np.array([[0.2,0.15,-0.01],[0.01,-0.1,-0.06],[0.14,-0.2,-0.03]])\n",
    "b1   = np.array([1.,1.,1.])\n",
    "z1   = W1.dot(a0) + b1\n",
    "a1   = np.tanh(z1)\n",
    "\n",
    "# Output layer\n",
    "W2   = np.array([[0.08,0.11,-0.3],[0.1,-0.15,0.08],[0.1,0.1,-0.07]])\n",
    "b2   = np.array([0.,1.,0.])\n",
    "z2   = W2.dot(a1) + b2\n",
    "a2 = y  = softmax(z2)\n",
    "imax = np.argmax(y)\n",
    "\n",
    "print ' z1 ',z1\n",
    "print ' a1 ',np.tanh(z1)\n",
    "print ' z2 ',z2\n",
    "print ' y  ',y\n",
    "print ' Input vector {0} is classified to label {1} '.format(a0,imax)\n",
    "\n",
    "\n",
    "print '\\n'\n",
    "for i in [0,1,2]:\n",
    "    print 'The probablity for classifying to label ',i,' is ',y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost (or error) functions \n",
    "\n",
    "Suppose that the expected output for an input vector ${\\bf x} \\equiv {\\bf a^1}$ is ${\\bf y} = {\\bf y_x}^* = (0,1,0)$, we can now compute the error vector ${\\bf e}= {\\bf e_x}= {\\bf a_x}^L-{\\bf y_x}^*$. With this error, we can now compute a cost $C=C_x$ assotiated with the output $\\bf{y_x}$ of the input vector ${\\bf x}$ (also called loss) function. For convinience of notations we will frequently omitt the subscript $x$.\n",
    "\n",
    "Popular loss functions are:\n",
    "* Absolute cost $C = C({\\bf a}^L)=\\sum_i |e_i|$\n",
    "* Square cost $C= C({\\bf a}^L) = \\sum_i e_i^2$\n",
    "* Cross entropy loss $C=C({\\bf a}^L) = -\\sum_i y_i^*\\log{a^L_i} \\equiv -\\sum_i y_i^*\\log{y_i}$. The rationale here is that the output of the softmax function is a probability distribution and we can also view the real label vector $y$ as a probability distribution (1 for the corerct label and 0 for all other labels). The cross entropy function is a common way to measure difference between distributions.\n",
    "\n",
    "The total error from all $N$ data vectors is computed as the average of the individual error terms associated with each input vector ${\\bf x}$, that is:$\\frac{1}{N} \\sum_x C_x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error                                           [ 0.18855564 -0.41801453  0.22945889]\n",
      " Absolute loss                                   0.836029061676\n",
      " Square loss                                     0.262940759619\n",
      " Cross entropy loss                              0.541309798638\n"
     ]
    }
   ],
   "source": [
    "def abs_loss(e):\n",
    "    return np.sum(np.abs(e))\n",
    "\n",
    "def sqr_loss(e):\n",
    "    return np.sum(e**2)\n",
    "\n",
    "def cross_entropy_loss(y_estimated,y_real):\n",
    "    return -np.sum(y_real*np.log(y_estimated))\n",
    "\n",
    "y_real = np.array([0.,1.,0])\n",
    "err    = a2 - ystar\n",
    "\n",
    "print ' Error                                          ',err\n",
    "print ' Absolute loss                                  ',abs_loss(err)\n",
    "print ' Square loss                                    ',sqr_loss(err)\n",
    "print ' Cross entropy loss                             ',cross_entropy_loss(a2,y_real)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Backpropagation is a fast way of computing the derivatives $\\frac{\\partial C}{\\partial w^l_{ij}}$ and $\\frac{\\partial C}{\\partial b_i}$ which are needed for the [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) procedure used for minimizing the cost function. Backpropagation is a special case of a more general technique called [reverse mode automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). The backpropagation algorithm is a smart application of the chain rule to allow efficient calculation of needed derivatives. A detailed discussion on the derivation of backpropagation if provided [in this tutorial](http://neuralnetworksanddeeplearning.com/chap2.html). An example for a simple python implementation is provided [in here](https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/).\n",
    "\n",
    "http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/\n",
    "\n",
    "\n",
    "#### Derivation of backpropagation\n",
    "Define the vector ${\\mathbf \\delta}^l= \\frac{\\partial C}{\\partial z^l}$, that is  $\\delta^l_i = \\frac{\\partial C}{\\partial z^l_i}, i =1,\\ldots,s_l$. \n",
    "\n",
    "Recall that $z^{l+1}_i = \\sum_j w^{l+1}_{ij} a^l_j + b^{l+1}_i = \\sum_j w^{l+1}_{ij} \\sigma_l(z^l_j) + b^{l+1}_i$ Then we have\n",
    "* $\\delta^L_i = \\frac{\\partial C}{\\partial z_i^L} = \\sum_k \\frac{\\partial C}{\\partial a_k^L}  \\frac{\\partial a_k^L}{\\partial z_i^L} = \\frac{\\partial C}{\\partial a_i^L}  \\frac{\\partial a_i^L}{\\partial z_i^L}=  \\frac{\\partial C}{\\partial a^L_i} \\sigma'_L ( z^L_i)$ \n",
    "\n",
    "* $\\delta^l_i = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}}  \\frac{\\partial z_k^{l+1}}{\\partial z_i^L}   =  \\sum_k \\delta_k^{l+1}  w_{ki}^{l+1} \\sigma'_l(z_i^l)        = \\sigma'_l(z_i^l) \\cdot ((W^{l+1})^T \\delta^{l+1})_i  $\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial b^l_{i}} =  \\delta^{l}_i$\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial w^l_{ij}} = \\frac{\\partial C}{\\partial z^{l}_{i}}    \\frac{\\partial z^{l}_{i}}{\\partial w^{l}_{ij}} = \\delta^{l}_i  a^{l-1}_j$\n",
    "\n",
    "#### In vector form:\n",
    "\n",
    "* $\\delta^L = \\frac{\\partial C}{\\partial {\\bf a}^L} \\odot $$\\sigma'_L ({\\bf z}^L)$ where $\\odot$ is the **Hadamard** elementwise product.\n",
    "\n",
    "* $\\delta^l = (W^{l+1})^T \\delta^{l+1} \\odot \\sigma'_l({\\bf z}^l)$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial b^l} =  \\delta^{l}$\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial w^l_{ij}} = \\delta^{l}_i  a^{l-1}_j$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://karpathy.github.io/neuralnets/\n",
    "\n",
    "\n",
    "\n",
    "Gradient descent example: http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\n",
    "\n",
    "SGD tricks: http://research.microsoft.com/pubs/192769/tricks-2012.pdf\n",
    "\n",
    "http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/\n",
    "\n",
    "\n",
    "http://code.activestate.com/recipes/578148-simple-back-propagation-neural-network-in-python-s/\n",
    "\n",
    "http://deeplearning.net/tutorial/\n",
    "\n",
    "\n",
    "http://stackoverflow.com/questions/15395835/simple-multi-layer-neural-network-implementation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
