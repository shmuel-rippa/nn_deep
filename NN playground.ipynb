{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## resources\n",
    "* [I am trask blog - simple introduction to NN](http://iamtrask.github.io/)\n",
    "* [Newral bnetwork tutorial](http://www.existor.com/en/news-neural-networks.html) - walk all the way\n",
    "* [Andrew Ng ML course](https://www.coursera.org/learn/machine-learning/)\n",
    "* [Pedro domingos course](https://www.coursera.org/course/machlearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work of a single neuron on a single example\n",
    "\n",
    "Consider a simple network with input units with nodes marked by $x_i$, output unit with nodes marked by $y_i$ and hidden units marked by $h_i$. A value of a specific node, say $h_1$ is evalueted from the values of all input nodes \n",
    "> $h_i = a(\\sum_{i=1}^{3} x_i W^{(1)}_{i1} x_i$ ) \n",
    " \n",
    "where $a$ is an **activation function**. That is we first computer a scalar product between the weights and the values of the input nodes. Then we use the compuyted value as the argument to the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scalar product between unit and weights  0.2\n",
      " Values of Sigmoid activation function    0.549833997312\n",
      " Values of tanh    activation function    0.197375320225\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def rectifier(x):\n",
    "    return np.array([max(xv,0.0) for xv in x])\n",
    "    \n",
    "\n",
    "x     = np.array([1.0,0,0])\n",
    "w     = np.array([0.2,-0.03,0.14])\n",
    "print ' Scalar product between unit and weights ',x.dot(w)\n",
    "print ' Values of Sigmoid activation function   ',sigmoid(x.dot(w))\n",
    "print ' Values of tanh    activation function   ',np.tanh(x.dot(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of activation functions\n",
    "\n",
    "Example of some popular [activation functions](https://en.wikipedia.org/wiki/Activation_function):\n",
    " * [Rectifier](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "\n",
    "x  = np.linspace(-2,2,100) # 100 linearly spaced numbers\n",
    "s  = sigmoid(x)    # computing the values of \n",
    "th = np.tanh(x)    # computing the values of \n",
    "re = rectifier(x)  # computing the values of rectifier\n",
    "\n",
    "# compose plot\n",
    "pylab.plot(x,s) \n",
    "pylab.plot(x,s,'co',label='Sigmoid') # Sigmoid \n",
    "pylab.plot(x,th,label='tanh')     # tanh\n",
    "pylab.plot(x,re,label='rectifier')     # tanh\n",
    "pylab.legend()\n",
    "pylab.show() # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix notations\n",
    "\n",
    "It is convenient to arrange the weights in a matrix so that all nodes can be computed at once:\n",
    "\n",
    "> $\\mathbf{h} = a(\\mathbf{W^{(1)}} \\cdot \\mathbf{x})$\n",
    "\n",
    "The output values are computed by similarly multplying the values oh $h$ by another weight matrix,\n",
    "\n",
    "> $\\mathbf{y} = \\mathbf{W^{(2)}} \\cdot \\mathbf{h}$\n",
    "\n",
    "We do not use the activation function this time so we actually use a **linear regressor** network. The values of $y$ arer arbitrary real numbers. For classification problems we want to ocnvert them into probabilities. This is achieved by using the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function.\n",
    "\n",
    "> $s_i = \\frac{e^{y_i}}{\\sum_i e^{y_i}}$\n",
    "\n",
    "The element $s_i$ is the probability that the label of the output is $i$. This is indeed the same expression utilized by [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) for classification of many labels.\n",
    "\n",
    "### Feed forward cycle for a simple network with input unit, one hidden unit and an output unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Value after scalar product (transfer function)  [ 0.2   0.15 -0.01]\n",
      " and after activation function                   [ 0.19737532  0.14888503 -0.00999967]\n",
      " output value                                    [ 0.02967856 -0.00162144 -0.04660182]\n",
      " softmax                                         [ 0.34533473  0.33469317  0.3199721 ]\n",
      "\n",
      "\n",
      "The probablity for classifying to label  0  is  0.345334733084\n",
      "The probablity for classifying to label  1  is  0.334693165649\n",
      "The probablity for classifying to label  2  is  0.319972101267\n"
     ]
    }
   ],
   "source": [
    "def softmax(y):\n",
    "    s = np.sum(np.exp(y))\n",
    "    return np.exp(y)/s\n",
    "\n",
    "x  = np.array([1.,0,0])\n",
    "W1 = np.array([[0.2,0.15,-0.01],[0.01,-0.1,-0.06],[0.14,-0.2,-0.03]])\n",
    "xw = W1.T.dot(x)\n",
    "h1 = np.tanh(xw)\n",
    "\n",
    "W2 = np.array([[0.08,0.11,-0.3],[0.1,-0.15,0.08],[0.1,0.1,-0.07]])\n",
    "y  = W2.T.dot(h1)\n",
    "s  = softmax(y)\n",
    "\n",
    "print ' Value after scalar product (transfer function) ',xw\n",
    "print ' and after activation function                  ',np.tanh(xw)\n",
    "print ' output value                                   ',y\n",
    "print ' softmax                                        ',s\n",
    "\n",
    "print '\\n'\n",
    "for i in [0,1,2]:\n",
    "    print 'The probablity for classifying to label ',i,' is ',s[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing errors\n",
    "Suppose that the expected output is $y^* = (0,1,0)$, we can now compute the error vector $e=s-y^*$. With this error, we can now compute a loss function. Popular loss functions are:\n",
    "* Absolute loss $\\sum_i |e_i|$\n",
    "* Square loss $\\sum_i e_i^2$\n",
    "* Cross entropy loss $-\\sum_i y_i^*\\log{s_i}$. The rationale here is that the output of the softmax function is a probability distribution and we can also view the real label vector $y$ as a probability distribution (1 for the corerct label and 0 for all other labels). The cross entropy function is a common way to measure difference between distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error                                           [ 0.34533473 -0.66530683  0.3199721 ]\n",
      " Absolute loss                                   1.3306136687\n",
      " Square loss                                     0.664271407297\n",
      " Cross entropy loss                              1.09454109031\n"
     ]
    }
   ],
   "source": [
    "def abs_loss(e):\n",
    "    return np.sum(np.abs(e))\n",
    "\n",
    "def sqr_loss(e):\n",
    "    return np.sum(e**2)\n",
    "\n",
    "def cross_entropy_loss(y_estimated,y_real):\n",
    "    return -np.sum(y_real*np.log(y_estimated))\n",
    "\n",
    "ystar = np.array([0.,1.,0])\n",
    "err   = s - ystar\n",
    "\n",
    "print ' Error                                          ',err\n",
    "print ' Absolute loss                                  ',abs_loss(err)\n",
    "print ' Square loss                                    ',sqr_loss(err)\n",
    "print ' Cross entropy loss                             ',cross_entropy_loss(s,ystar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work of a single neuron on 4 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data matrix (4 examples in 3 dimensions)\n",
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "\n",
      " The target vector y\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Random weights \n",
      "[[  4.17022005e-01]\n",
      " [  7.20324493e-01]\n",
      " [  1.14374817e-04]]\n",
      "\n",
      "Values of activation function\n",
      "[[ 0.50002859]\n",
      " [ 0.67270365]\n",
      " [ 0.60279781]\n",
      " [ 0.75721315]]\n"
     ]
    }
   ],
   "source": [
    "X     = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y     = np.array([[0,1,1,0]]).T\n",
    "\n",
    "np.random.seed(1)\n",
    "w0 = np.random.random((3,1))\n",
    "\n",
    "print 'The data matrix (4 examples in 3 dimensions)\\n',X\n",
    "print '\\n The target vector y\\n',y\n",
    "print '\\nRandom weights \\n',w0\n",
    "print '\\nValues of activation function\\n',sigmoid(np.dot(X,w0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "X     = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y     = np.array([[0,0,1,1]]).T\n",
    "\n",
    "np.random.seed(1)\n",
    "w0 = 2.0*np.random.random((3,1))-1.\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0,w0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * sigmoid(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    w0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print l1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
